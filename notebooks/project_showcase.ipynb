{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Detection of Hate Speech on Croatian Online Portals Using NLP Methods\n\n**Authors:** Duje Jurić, Teo Matošević, Teo Radolović\n\n**Institution:** University of Zagreb, Faculty of Electrical Engineering and Computing\n\n**Course:** Natural Language Processing (Obrada prirodnog jezika)\n\n---\n\nThis notebook provides a comprehensive overview of our Croatian hate speech detection project, demonstrating all major components including data exploration, lexicon analysis, model training, evaluation, and interactive demos.\n\n**Pre-trained Models (HuggingFace):**\n- BERTić: [TeoMatosevic/croatian-hate-speech-bertic](https://huggingface.co/TeoMatosevic/croatian-hate-speech-bertic)\n- Baseline: [TeoMatosevic/croatian-hate-speech-baseline](https://huggingface.co/TeoMatosevic/croatian-hate-speech-baseline)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, '..')\n",
    "os.chdir('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Project Overview\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "The proliferation of hate speech on Croatian online platforms presents a significant challenge for content moderation. This project investigates the effectiveness of NLP methods for automated detection of offensive language in Croatian.\n",
    "\n",
    "### Research Questions\n",
    "\n",
    "1. **RQ1:** How effective are transformer-based models compared to traditional ML approaches for Croatian hate speech detection?\n",
    "2. **RQ2:** Can models pre-trained on related South Slavic languages transfer effectively to Croatian offensive language detection?\n",
    "\n",
    "### Key Results\n",
    "\n",
    "| Model | Accuracy | F1-Macro | Improvement |\n",
    "|-------|----------|----------|-------------|\n",
    "| TF-IDF + Logistic Regression | 69.0% | 0.684 | Baseline |\n",
    "| TF-IDF + SVM | 68.5% | 0.680 | -0.6% |\n",
    "| **BERTić (fine-tuned)** | **81.3%** | **0.810** | **+18.5%** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Exploration\n",
    "\n",
    "We use the FRENK Croatian hate speech dataset containing 10,971 annotated comments from Croatian news portals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_df = pd.read_json('data/processed/frenk_train.jsonl', lines=True)\n",
    "dev_df = pd.read_json('data/processed/frenk_dev.jsonl', lines=True)\n",
    "test_df = pd.read_json('data/processed/frenk_test.jsonl', lines=True)\n",
    "\n",
    "# Combine for full analysis\n",
    "all_df = pd.concat([train_df, dev_df, test_df], ignore_index=True)\n",
    "\n",
    "print(f\"Dataset Statistics:\")\n",
    "print(f\"{'='*40}\")\n",
    "print(f\"Training set:   {len(train_df):,} samples\")\n",
    "print(f\"Development set: {len(dev_df):,} samples\")\n",
    "print(f\"Test set:       {len(test_df):,} samples\")\n",
    "print(f\"{'='*40}\")\n",
    "print(f\"Total:          {len(all_df):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample data\n",
    "print(\"Sample comments from the dataset:\")\n",
    "print(\"=\"*60)\n",
    "all_df[['text', 'label']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart\n",
    "label_counts = all_df['label'].value_counts()\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "axes[0].pie(label_counts, labels=['Acceptable (ACC)', 'Offensive (OFF)'], \n",
    "            autopct='%1.1f%%', colors=colors, explode=(0.02, 0.02),\n",
    "            shadow=True, startangle=90)\n",
    "axes[0].set_title('Label Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bar chart by split\n",
    "split_data = pd.DataFrame({\n",
    "    'Split': ['Train', 'Train', 'Dev', 'Dev', 'Test', 'Test'],\n",
    "    'Label': ['ACC', 'OFF', 'ACC', 'OFF', 'ACC', 'OFF'],\n",
    "    'Count': [\n",
    "        len(train_df[train_df['label'] == 'ACC']),\n",
    "        len(train_df[train_df['label'] == 'OFF']),\n",
    "        len(dev_df[dev_df['label'] == 'ACC']),\n",
    "        len(dev_df[dev_df['label'] == 'OFF']),\n",
    "        len(test_df[test_df['label'] == 'ACC']),\n",
    "        len(test_df[test_df['label'] == 'OFF'])\n",
    "    ]\n",
    "})\n",
    "sns.barplot(data=split_data, x='Split', y='Count', hue='Label', ax=axes[1], palette=colors)\n",
    "axes[1].set_title('Label Distribution by Split', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Number of Samples')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nClass distribution: ACC={label_counts.get('ACC', 0):,} ({label_counts.get('ACC', 0)/len(all_df)*100:.1f}%), \"\n",
    "      f\"OFF={label_counts.get('OFF', 0):,} ({label_counts.get('OFF', 0)/len(all_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length analysis\n",
    "all_df['text_length'] = all_df['text'].str.len()\n",
    "all_df['word_count'] = all_df['text'].str.split().str.len()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Character length distribution\n",
    "for label, color in zip(['ACC', 'OFF'], colors):\n",
    "    subset = all_df[all_df['label'] == label]['text_length']\n",
    "    axes[0].hist(subset, bins=50, alpha=0.6, label=label, color=color)\n",
    "axes[0].set_xlabel('Character Length')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Text Length Distribution by Class', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Word count distribution\n",
    "for label, color in zip(['ACC', 'OFF'], colors):\n",
    "    subset = all_df[all_df['label'] == label]['word_count']\n",
    "    axes[1].hist(subset, bins=50, alpha=0.6, label=label, color=color)\n",
    "axes[1].set_xlabel('Word Count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Word Count Distribution by Class', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nText Statistics:\")\n",
    "print(f\"  Average length: {all_df['text_length'].mean():.1f} characters\")\n",
    "print(f\"  Average words:  {all_df['word_count'].mean():.1f} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample comments from each class\n",
    "print(\"Sample ACCEPTABLE (ACC) Comments:\")\n",
    "print(\"=\"*60)\n",
    "for i, row in all_df[all_df['label'] == 'ACC'].sample(5).iterrows():\n",
    "    print(f\"  - {row['text'][:100]}...\" if len(row['text']) > 100 else f\"  - {row['text']}\")\n",
    "\n",
    "print(\"\\nSample OFFENSIVE (OFF) Comments:\")\n",
    "print(\"=\"*60)\n",
    "for i, row in all_df[all_df['label'] == 'OFF'].sample(5).iterrows():\n",
    "    print(f\"  - {row['text'][:100]}...\" if len(row['text']) > 100 else f\"  - {row['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Coded Language Lexicon\n",
    "\n",
    "We developed a lexicon of 32 \"dog whistle\" terms - seemingly innocuous words used with hidden hateful meanings in Croatian online discourse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lexicon\n",
    "with open('data/lexicon/coded_terms.json', 'r', encoding='utf-8') as f:\n",
    "    lexicon_data = json.load(f)\n",
    "\n",
    "# Combine main and user-provided terms\n",
    "all_terms = lexicon_data.get('coded_terms', []) + lexicon_data.get('user_provided_terms', [])\n",
    "\n",
    "print(f\"Lexicon Statistics:\")\n",
    "print(f\"{'='*40}\")\n",
    "print(f\"Total coded terms: {len(all_terms)}\")\n",
    "print(f\"Main terms: {len(lexicon_data.get('coded_terms', []))}\")\n",
    "print(f\"User-provided terms: {len(lexicon_data.get('user_provided_terms', []))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for visualization\n",
    "terms_df = pd.DataFrame(all_terms)\n",
    "\n",
    "# Show sample entries\n",
    "print(\"\\nSample Coded Terms:\")\n",
    "print(\"=\"*80)\n",
    "display_cols = ['term', 'literal_meaning', 'coded_meaning', 'target_group']\n",
    "available_cols = [c for c in display_cols if c in terms_df.columns]\n",
    "terms_df[available_cols].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize terms by target group\n",
    "if 'target_group' in terms_df.columns:\n",
    "    target_counts = terms_df['target_group'].value_counts()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.barh(target_counts.index, target_counts.values, color=sns.color_palette('husl', len(target_counts)))\n",
    "    plt.xlabel('Number of Terms')\n",
    "    plt.ylabel('Target Group')\n",
    "    plt.title('Coded Terms by Target Group', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, count in zip(bars, target_counts.values):\n",
    "        plt.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2, \n",
    "                 str(count), va='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate lexicon matching\n",
    "from src.utils.lexicon import CodedTermLexicon\n",
    "\n",
    "lexicon = CodedTermLexicon('data/lexicon/coded_terms.json')\n",
    "\n",
    "test_sentences = [\n",
    "    \"Inženjeri opet prave nered u gradu.\",\n",
    "    \"Globalisti kontroliraju sve medije.\",\n",
    "    \"Ovo je normalan komentar bez kodiranih riječi.\",\n",
    "    \"Ovce će primiti sve što im kažu.\",\n",
    "    \"Kulturno obogaćenje nam donosi samo probleme.\"\n",
    "]\n",
    "\n",
    "print(\"Lexicon Matching Demo:\")\n",
    "print(\"=\"*70)\n",
    "for sentence in test_sentences:\n",
    "    matches = lexicon.find_matches(sentence)\n",
    "    print(f\"\\nText: \\\"{sentence}\\\"\")\n",
    "    if matches:\n",
    "        for match in matches:\n",
    "            print(f\"  Found: '{match['term']}' -> {match['coded_meaning']} (Target: {match['target_group']})\")\n",
    "    else:\n",
    "        print(\"  No coded terms found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Baseline Models (TF-IDF + Classical ML)\n\nWe implement TF-IDF vectorization with Logistic Regression and SVM classifiers as baselines.\n\n**Pre-trained baseline model:** [TeoMatosevic/croatian-hate-speech-baseline](https://huggingface.co/TeoMatosevic/croatian-hate-speech-baseline)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.baseline import BaselineClassifier\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if model exists\n",
    "baseline_path = Path('checkpoints/baseline/logistic_regression_model.pkl')\n",
    "\n",
    "if baseline_path.exists():\n",
    "    print(\"Loading pre-trained baseline model...\")\n",
    "    baseline = BaselineClassifier.load(str(baseline_path))\n",
    "    print(\"Baseline model loaded successfully!\")\n",
    "else:\n",
    "    print(\"Training new baseline model...\")\n",
    "    baseline = BaselineClassifier(classifier_type='logistic_regression')\n",
    "    baseline.fit(train_df['text'].tolist(), train_df['label'].tolist())\n",
    "    print(\"Baseline model trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline on test set\n",
    "baseline_results = baseline.evaluate(test_df['text'].tolist(), test_df['label'].tolist())\n",
    "\n",
    "print(\"Baseline Model Performance (Test Set):\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy:    {baseline_results.get('accuracy', 0):.1%}\")\n",
    "print(f\"F1-Macro:    {baseline_results.get('f1_macro', 0):.3f}\")\n",
    "print(f\"F1-Weighted: {baseline_results.get('f1_weighted', 0):.3f}\")\n",
    "print(f\"Precision:   {baseline_results.get('precision_macro', 0):.3f}\")\n",
    "print(f\"Recall:      {baseline_results.get('recall_macro', 0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance - top words for each class\n",
    "if hasattr(baseline, 'get_feature_importance'):\n",
    "    importance = baseline.get_feature_importance(top_n=15)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    for idx, (label, features) in enumerate(importance.items()):\n",
    "        words = [f[0] for f in features]\n",
    "        weights = [f[1] for f in features]\n",
    "        \n",
    "        color = '#2ecc71' if label == 'ACC' else '#e74c3c'\n",
    "        axes[idx].barh(words, weights, color=color)\n",
    "        axes[idx].set_xlabel('Weight')\n",
    "        axes[idx].set_title(f'Top Words for {label}', fontsize=14, fontweight='bold')\n",
    "        axes[idx].invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for baseline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "baseline_preds = baseline.predict(test_df['text'].tolist())\n",
    "cm = confusion_matrix(test_df['label'].tolist(), baseline_preds, labels=['ACC', 'OFF'])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['ACC', 'OFF'], yticklabels=['ACC', 'OFF'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Baseline Model Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. BERTić Transformer Model\n\nWe fine-tune BERTić (`classla/bcms-bertic`), a BERT model pre-trained on 8 billion tokens of South Slavic text.\n\n**Our fine-tuned model:** [TeoMatosevic/croatian-hate-speech-bertic](https://huggingface.co/TeoMatosevic/croatian-hate-speech-bertic)\n\nTo download:\n```python\nfrom huggingface_hub import snapshot_download\nsnapshot_download(\"TeoMatosevic/croatian-hate-speech-bertic\", local_dir=\"checkpoints/bertic/best_model\")\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BERTić Model Configuration:\")\n",
    "print(\"=\"*50)\n",
    "print(\"Base model:       classla/bcms-bertic\")\n",
    "print(\"Architecture:     12 layers, 768 hidden dim, 110M params\")\n",
    "print(\"\")\n",
    "print(\"Training Configuration:\")\n",
    "print(\"  Learning rate:  2e-5\")\n",
    "print(\"  Batch size:     16\")\n",
    "print(\"  Epochs:         5\")\n",
    "print(\"  Max length:     256 tokens\")\n",
    "print(\"  Optimizer:      AdamW\")\n",
    "print(\"  Warmup ratio:   0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERTić if available\n",
    "from src.models.bertic import BERTicTrainer\n",
    "\n",
    "bertic_path = Path('checkpoints/bertic/best_model')\n",
    "bertic = None\n",
    "\n",
    "if bertic_path.exists():\n",
    "    print(\"Loading pre-trained BERTić model...\")\n",
    "    try:\n",
    "        bertic = BERTicTrainer()\n",
    "        bertic.load(str(bertic_path))\n",
    "        print(\"BERTić model loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load BERTić: {e}\")\n",
    "        print(\"BERTić results will be shown from saved metrics.\")\n",
    "else:\n",
    "    print(\"BERTić checkpoint not found.\")\n",
    "    print(\"Results shown below are from previous training runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTić results (from training)\n",
    "bertic_results = {\n",
    "    'accuracy': 0.8127,\n",
    "    'f1_macro': 0.810,\n",
    "    'f1_weighted': 0.813,\n",
    "    'mcc': 0.621,\n",
    "    'per_class': {\n",
    "        'ACC': {'precision': 0.777, 'recall': 0.803, 'f1': 0.790},\n",
    "        'OFF': {'precision': 0.842, 'recall': 0.820, 'f1': 0.831}\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"BERTić Model Performance (Test Set):\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy:    {bertic_results['accuracy']:.1%}\")\n",
    "print(f\"F1-Macro:    {bertic_results['f1_macro']:.3f}\")\n",
    "print(f\"F1-Weighted: {bertic_results['f1_weighted']:.3f}\")\n",
    "print(f\"MCC:         {bertic_results['mcc']:.3f}\")\n",
    "print(\"\\nPer-Class Performance:\")\n",
    "print(f\"  ACC: P={bertic_results['per_class']['ACC']['precision']:.3f}, \"\n",
    "      f\"R={bertic_results['per_class']['ACC']['recall']:.3f}, \"\n",
    "      f\"F1={bertic_results['per_class']['ACC']['f1']:.3f}\")\n",
    "print(f\"  OFF: P={bertic_results['per_class']['OFF']['precision']:.3f}, \"\n",
    "      f\"R={bertic_results['per_class']['OFF']['recall']:.3f}, \"\n",
    "      f\"F1={bertic_results['per_class']['OFF']['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTić per-class metrics visualization\n",
    "metrics_data = {\n",
    "    'Class': ['ACC', 'ACC', 'ACC', 'OFF', 'OFF', 'OFF'],\n",
    "    'Metric': ['Precision', 'Recall', 'F1-Score', 'Precision', 'Recall', 'F1-Score'],\n",
    "    'Value': [0.777, 0.803, 0.790, 0.842, 0.820, 0.831]\n",
    "}\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=metrics_df, x='Class', y='Value', hue='Metric', palette='viridis')\n",
    "plt.ylim(0.7, 0.9)\n",
    "plt.title('BERTić Per-Class Performance', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Score')\n",
    "plt.legend(title='Metric')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison\n",
    "\n",
    "Comparing all models to highlight the improvement from transformer-based approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "comparison_data = {\n",
    "    'Model': ['Logistic Regression', 'SVM (Linear)', 'BERTić'],\n",
    "    'Accuracy': [0.690, 0.685, 0.813],\n",
    "    'F1-Macro': [0.684, 0.680, 0.810],\n",
    "    'F1-Weighted': [0.689, 0.684, 0.813],\n",
    "    'MCC': [0.371, 0.361, 0.621]\n",
    "}\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(\"=\"*70)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart comparison\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.2\n",
    "\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6']\n",
    "metrics = ['Accuracy', 'F1-Macro', 'F1-Weighted', 'MCC']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    axes[0].bar(x + i*width, comparison_df[metric], width, label=metric, color=colors[i])\n",
    "\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x + width * 1.5)\n",
    "axes[0].set_xticklabels(comparison_df['Model'], rotation=15)\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "# Improvement chart\n",
    "baseline_f1 = 0.684\n",
    "improvements = [(f1 - baseline_f1) / baseline_f1 * 100 for f1 in comparison_df['F1-Macro']]\n",
    "colors_imp = ['gray', 'gray', '#27ae60']\n",
    "bars = axes[1].bar(comparison_df['Model'], improvements, color=colors_imp)\n",
    "axes[1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[1].set_ylabel('Improvement over Baseline (%)')\n",
    "axes[1].set_title('F1-Macro Improvement', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticklabels(comparison_df['Model'], rotation=15)\n",
    "\n",
    "# Add value labels\n",
    "for bar, imp in zip(bars, improvements):\n",
    "    if imp > 0:\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                     f'+{imp:.1f}%', ha='center', fontweight='bold', color='green')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nKey Finding: BERTić achieves +18.5% F1 improvement over baseline!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Demo\n",
    "\n",
    "Analyze any text with our models and lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(text, baseline_model=None, bertic_model=None, lexicon=None):\n",
    "    \"\"\"Analyze text with all available models.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(f\"INPUT: \\\"{text}\\\"\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Lexicon analysis\n",
    "    if lexicon:\n",
    "        matches = lexicon.find_matches(text)\n",
    "        print(\"\\n[LEXICON] Coded Terms:\")\n",
    "        if matches:\n",
    "            for match in matches:\n",
    "                print(f\"  - '{match['term']}' -> {match['coded_meaning']} (Target: {match['target_group']})\")\n",
    "        else:\n",
    "            print(\"  No coded terms detected.\")\n",
    "    \n",
    "    # Baseline prediction\n",
    "    if baseline_model:\n",
    "        pred = baseline_model.predict([text])[0]\n",
    "        proba = baseline_model.predict_proba([text])\n",
    "        print(f\"\\n[BASELINE] Prediction: {pred}\")\n",
    "        if proba is not None:\n",
    "            print(f\"  Confidence: {proba[0].max():.1%}\")\n",
    "    \n",
    "    # BERTić prediction\n",
    "    if bertic_model:\n",
    "        try:\n",
    "            pred = bertic_model.predict([text])[0]\n",
    "            print(f\"\\n[BERTić] Prediction: {pred}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n[BERTić] Not available: {e}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo with example sentences\n",
    "demo_sentences = [\n",
    "    \"Hvala na lijepom komentaru, slažem se s vama.\",\n",
    "    \"Inženjeri opet prave probleme u našem gradu.\",\n",
    "    \"Svi političari su lopovi i treba ih zatvoriti.\",\n",
    "    \"Globalisti žele uništiti našu kulturu i tradiciju.\",\n",
    "    \"Ovo je odlična vijest za Hrvatsku!\",\n",
    "    \"Ovce će vjerovati u sve što im mediji kažu.\"\n",
    "]\n",
    "\n",
    "print(\"Demo Analysis of Sample Sentences:\\n\")\n",
    "for sentence in demo_sentences:\n",
    "    analyze_text(sentence, baseline_model=baseline, bertic_model=bertic, lexicon=lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Error Analysis\n",
    "\n",
    "Examining cases where the model makes mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified examples (baseline)\n",
    "test_texts = test_df['text'].tolist()\n",
    "test_labels = test_df['label'].tolist()\n",
    "baseline_predictions = baseline.predict(test_texts)\n",
    "\n",
    "# Create error analysis dataframe\n",
    "error_df = test_df.copy()\n",
    "error_df['predicted'] = baseline_predictions\n",
    "error_df['correct'] = error_df['label'] == error_df['predicted']\n",
    "\n",
    "misclassified = error_df[~error_df['correct']]\n",
    "print(f\"Baseline Misclassification Analysis:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total test samples: {len(test_df)}\")\n",
    "print(f\"Correctly classified: {len(error_df[error_df['correct']])} ({len(error_df[error_df['correct']])/len(test_df)*100:.1f}%)\")\n",
    "print(f\"Misclassified: {len(misclassified)} ({len(misclassified)/len(test_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample misclassified examples\n",
    "print(\"\\nSample False Positives (ACC predicted as OFF):\")\n",
    "print(\"-\"*60)\n",
    "false_positives = misclassified[(misclassified['label'] == 'ACC') & (misclassified['predicted'] == 'OFF')]\n",
    "for i, row in false_positives.head(3).iterrows():\n",
    "    print(f\"  Text: {row['text'][:80]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nSample False Negatives (OFF predicted as ACC):\")\n",
    "print(\"-\"*60)\n",
    "false_negatives = misclassified[(misclassified['label'] == 'OFF') & (misclassified['predicted'] == 'ACC')]\n",
    "for i, row in false_negatives.head(3).iterrows():\n",
    "    print(f\"  Text: {row['text'][:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if lexicon could help with misclassified examples\n",
    "print(\"Lexicon Analysis of Misclassified Examples:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lexicon_could_help = 0\n",
    "for i, row in misclassified.sample(min(20, len(misclassified))).iterrows():\n",
    "    matches = lexicon.find_matches(row['text'])\n",
    "    if matches and row['label'] == 'OFF' and row['predicted'] == 'ACC':\n",
    "        lexicon_could_help += 1\n",
    "        print(f\"\\nText: {row['text'][:60]}...\")\n",
    "        print(f\"  True: {row['label']}, Predicted: {row['predicted']}\")\n",
    "        print(f\"  Coded terms found: {[m['term'] for m in matches]}\")\n",
    "\n",
    "print(f\"\\n{lexicon_could_help} false negatives contained coded terms that could aid detection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10. Conclusions\n\n### Key Findings\n\n1. **Transformer Superiority**: BERTić significantly outperforms traditional ML baselines, achieving +18.5% F1 improvement.\n\n2. **Transfer Learning Effectiveness**: Pre-training on related South Slavic languages enables effective knowledge transfer to Croatian hate speech detection.\n\n3. **Balanced Performance**: BERTić achieves balanced precision and recall across both classes (ACC F1=0.790, OFF F1=0.831).\n\n4. **Coded Language Detection**: The lexicon of 32 dog whistle terms provides complementary detection for implicit hate speech.\n\n### Practical Applications\n\n- Semi-automated content moderation on Croatian online platforms\n- Reducing manual moderation workload while maintaining accuracy\n- Detecting implicit hate speech through lexicon integration\n\n### Future Work\n\n- Multi-label classification for hate speech subtypes\n- Cross-platform evaluation (social media)\n- Integration of lexicon features with neural models\n- Explainable AI for transparent moderation decisions\n\n---\n\n**Repository:** https://github.com/TeoMatosevic/slur-analysis-model\n\n**Pre-trained Models (HuggingFace):**\n- BERTić: https://huggingface.co/TeoMatosevic/croatian-hate-speech-bertic\n- Baseline: https://huggingface.co/TeoMatosevic/croatian-hate-speech-baseline"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*60)\nprint(\"PROJECT SHOWCASE COMPLETE\")\nprint(\"=\"*60)\nprint(\"\\nThis notebook demonstrated all major components of the\")\nprint(\"Croatian Hate Speech Detection project.\")\nprint(\"\\nFor more information, see:\")\nprint(\"  - docs/paper.md (Academic paper)\")\nprint(\"  - README.md (Project documentation)\")\nprint(\"  - src/demo.py (Interactive demo script)\")\nprint(\"\\nPre-trained Models (HuggingFace):\")\nprint(\"  - BERTić: huggingface.co/TeoMatosevic/croatian-hate-speech-bertic\")\nprint(\"  - Baseline: huggingface.co/TeoMatosevic/croatian-hate-speech-baseline\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}