{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Detection of Hate Speech on Croatian Online Portals Using NLP Methods\n\n**Authors:** Duje Jurić, Teo Matošević, Teo Radolović\n\n**Institution:** University of Zagreb, Faculty of Electrical Engineering and Computing\n\n**Course:** Natural Language Processing (Obrada prirodnog jezika)\n\n---\n\nThis notebook provides a comprehensive overview of our Croatian hate speech detection project, demonstrating all major components including data exploration, lexicon analysis, model training, evaluation, and interactive demos.\n\n**Pre-trained Models (HuggingFace):**\n- BERTić: [TeoMatosevic/croatian-hate-speech-bertic](https://huggingface.co/TeoMatosevic/croatian-hate-speech-bertic)\n- XLM-RoBERTa: [TeoMatosevic/croatian-hate-speech-xlm-roberta](https://huggingface.co/TeoMatosevic/croatian-hate-speech-xlm-roberta)\n- Baseline: [TeoMatosevic/croatian-hate-speech-baseline](https://huggingface.co/TeoMatosevic/croatian-hate-speech-baseline)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, '..')\n",
    "os.chdir('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Project Overview\n\n### Problem Statement\n\nThe proliferation of hate speech on Croatian online platforms presents a significant challenge for content moderation. This project investigates the effectiveness of NLP methods for automated detection of offensive language in Croatian.\n\n### Research Questions\n\n1. **RQ1:** How effective are transformer-based models compared to traditional ML approaches for Croatian hate speech detection?\n2. **RQ2:** Can models pre-trained on related South Slavic languages transfer effectively to Croatian offensive language detection?\n\n### Key Results\n\n| Model | Accuracy | F1-Macro | MCC |\n|-------|----------|----------|-----|\n| TF-IDF + Logistic Regression | 71.6% | 0.711 | 0.423 |\n| TF-IDF + SVM | 71.0% | 0.707 | 0.414 |\n| XLM-RoBERTa (fine-tuned) | 74.8% | 0.745 | 0.490 |\n| **BERTić (fine-tuned)** | **81.3%** | **0.810** | **0.621** |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Exploration\n",
    "\n",
    "We use the FRENK Croatian hate speech dataset containing 10,971 annotated comments from Croatian news portals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_df = pd.read_json('data/processed/frenk_train.jsonl', lines=True)\n",
    "dev_df = pd.read_json('data/processed/frenk_dev.jsonl', lines=True)\n",
    "test_df = pd.read_json('data/processed/frenk_test.jsonl', lines=True)\n",
    "\n",
    "# Combine for full analysis\n",
    "all_df = pd.concat([train_df, dev_df, test_df], ignore_index=True)\n",
    "\n",
    "print(f\"Dataset Statistics:\")\n",
    "print(f\"{'='*40}\")\n",
    "print(f\"Training set:   {len(train_df):,} samples\")\n",
    "print(f\"Development set: {len(dev_df):,} samples\")\n",
    "print(f\"Test set:       {len(test_df):,} samples\")\n",
    "print(f\"{'='*40}\")\n",
    "print(f\"Total:          {len(all_df):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample data\n",
    "print(\"Sample comments from the dataset:\")\n",
    "print(\"=\"*60)\n",
    "all_df[['text', 'label']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart\n",
    "label_counts = all_df['label'].value_counts()\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "axes[0].pie(label_counts, labels=['Acceptable (ACC)', 'Offensive (OFF)'], \n",
    "            autopct='%1.1f%%', colors=colors, explode=(0.02, 0.02),\n",
    "            shadow=True, startangle=90)\n",
    "axes[0].set_title('Label Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bar chart by split\n",
    "split_data = pd.DataFrame({\n",
    "    'Split': ['Train', 'Train', 'Dev', 'Dev', 'Test', 'Test'],\n",
    "    'Label': ['ACC', 'OFF', 'ACC', 'OFF', 'ACC', 'OFF'],\n",
    "    'Count': [\n",
    "        len(train_df[train_df['label'] == 'ACC']),\n",
    "        len(train_df[train_df['label'] == 'OFF']),\n",
    "        len(dev_df[dev_df['label'] == 'ACC']),\n",
    "        len(dev_df[dev_df['label'] == 'OFF']),\n",
    "        len(test_df[test_df['label'] == 'ACC']),\n",
    "        len(test_df[test_df['label'] == 'OFF'])\n",
    "    ]\n",
    "})\n",
    "sns.barplot(data=split_data, x='Split', y='Count', hue='Label', ax=axes[1], palette=colors)\n",
    "axes[1].set_title('Label Distribution by Split', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Number of Samples')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nClass distribution: ACC={label_counts.get('ACC', 0):,} ({label_counts.get('ACC', 0)/len(all_df)*100:.1f}%), \"\n",
    "      f\"OFF={label_counts.get('OFF', 0):,} ({label_counts.get('OFF', 0)/len(all_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length analysis\n",
    "all_df['text_length'] = all_df['text'].str.len()\n",
    "all_df['word_count'] = all_df['text'].str.split().str.len()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Character length distribution\n",
    "for label, color in zip(['ACC', 'OFF'], colors):\n",
    "    subset = all_df[all_df['label'] == label]['text_length']\n",
    "    axes[0].hist(subset, bins=50, alpha=0.6, label=label, color=color)\n",
    "axes[0].set_xlabel('Character Length')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Text Length Distribution by Class', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Word count distribution\n",
    "for label, color in zip(['ACC', 'OFF'], colors):\n",
    "    subset = all_df[all_df['label'] == label]['word_count']\n",
    "    axes[1].hist(subset, bins=50, alpha=0.6, label=label, color=color)\n",
    "axes[1].set_xlabel('Word Count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Word Count Distribution by Class', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nText Statistics:\")\n",
    "print(f\"  Average length: {all_df['text_length'].mean():.1f} characters\")\n",
    "print(f\"  Average words:  {all_df['word_count'].mean():.1f} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample comments from each class\n",
    "print(\"Sample ACCEPTABLE (ACC) Comments:\")\n",
    "print(\"=\"*60)\n",
    "for i, row in all_df[all_df['label'] == 'ACC'].sample(5).iterrows():\n",
    "    print(f\"  - {row['text'][:100]}...\" if len(row['text']) > 100 else f\"  - {row['text']}\")\n",
    "\n",
    "print(\"\\nSample OFFENSIVE (OFF) Comments:\")\n",
    "print(\"=\"*60)\n",
    "for i, row in all_df[all_df['label'] == 'OFF'].sample(5).iterrows():\n",
    "    print(f\"  - {row['text'][:100]}...\" if len(row['text']) > 100 else f\"  - {row['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Coded Language Lexicon\n",
    "\n",
    "We developed a lexicon of 32 \"dog whistle\" terms - seemingly innocuous words used with hidden hateful meanings in Croatian online discourse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lexicon\n",
    "with open('data/lexicon/coded_terms.json', 'r', encoding='utf-8') as f:\n",
    "    lexicon_data = json.load(f)\n",
    "\n",
    "# Combine main and user-provided terms\n",
    "all_terms = lexicon_data.get('coded_terms', []) + lexicon_data.get('user_provided_terms', [])\n",
    "\n",
    "print(f\"Lexicon Statistics:\")\n",
    "print(f\"{'='*40}\")\n",
    "print(f\"Total coded terms: {len(all_terms)}\")\n",
    "print(f\"Main terms: {len(lexicon_data.get('coded_terms', []))}\")\n",
    "print(f\"User-provided terms: {len(lexicon_data.get('user_provided_terms', []))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for visualization\n",
    "terms_df = pd.DataFrame(all_terms)\n",
    "\n",
    "# Show sample entries\n",
    "print(\"\\nSample Coded Terms:\")\n",
    "print(\"=\"*80)\n",
    "display_cols = ['term', 'literal_meaning', 'coded_meaning', 'target_group']\n",
    "available_cols = [c for c in display_cols if c in terms_df.columns]\n",
    "terms_df[available_cols].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize terms by target group\n",
    "if 'target_group' in terms_df.columns:\n",
    "    target_counts = terms_df['target_group'].value_counts()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.barh(target_counts.index, target_counts.values, color=sns.color_palette('husl', len(target_counts)))\n",
    "    plt.xlabel('Number of Terms')\n",
    "    plt.ylabel('Target Group')\n",
    "    plt.title('Coded Terms by Target Group', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, count in zip(bars, target_counts.values):\n",
    "        plt.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2, \n",
    "                 str(count), va='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate lexicon matching\n",
    "from src.utils.lexicon import CodedTermLexicon\n",
    "\n",
    "lexicon = CodedTermLexicon('data/lexicon/coded_terms.json')\n",
    "\n",
    "test_sentences = [\n",
    "    \"Inženjeri opet prave nered u gradu.\",\n",
    "    \"Globalisti kontroliraju sve medije.\",\n",
    "    \"Ovo je normalan komentar bez kodiranih riječi.\",\n",
    "    \"Ovce će primiti sve što im kažu.\",\n",
    "    \"Kulturno obogaćenje nam donosi samo probleme.\"\n",
    "]\n",
    "\n",
    "print(\"Lexicon Matching Demo:\")\n",
    "print(\"=\"*70)\n",
    "for sentence in test_sentences:\n",
    "    matches = lexicon.find_matches(sentence)\n",
    "    print(f\"\\nText: \\\"{sentence}\\\"\")\n",
    "    if matches:\n",
    "        for match in matches:\n",
    "            print(f\"  Found: '{match['term']}' -> {match['coded_meaning']} (Target: {match['target_group']})\")\n",
    "    else:\n",
    "        print(\"  No coded terms found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Baseline Models (TF-IDF + Classical ML)\n\nWe implement TF-IDF vectorization with Logistic Regression and SVM classifiers as baselines.\n\n**Pre-trained baseline model:** [TeoMatosevic/croatian-hate-speech-baseline](https://huggingface.co/TeoMatosevic/croatian-hate-speech-baseline)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.baseline import BaselineClassifier\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if model exists\n",
    "baseline_path = Path('checkpoints/baseline/logistic_regression_model.pkl')\n",
    "\n",
    "if baseline_path.exists():\n",
    "    print(\"Loading pre-trained baseline model...\")\n",
    "    baseline = BaselineClassifier.load(str(baseline_path))\n",
    "    print(\"Baseline model loaded successfully!\")\n",
    "else:\n",
    "    print(\"Training new baseline model...\")\n",
    "    baseline = BaselineClassifier(classifier_type='logistic_regression')\n",
    "    baseline.fit(train_df['text'].tolist(), train_df['label'].tolist())\n",
    "    print(\"Baseline model trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline on test set\n",
    "baseline_results = baseline.evaluate(test_df['text'].tolist(), test_df['label'].tolist())\n",
    "\n",
    "print(\"Baseline Model Performance (Test Set):\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy:    {baseline_results.get('accuracy', 0):.1%}\")\n",
    "print(f\"F1-Macro:    {baseline_results.get('f1_macro', 0):.3f}\")\n",
    "print(f\"F1-Weighted: {baseline_results.get('f1_weighted', 0):.3f}\")\n",
    "print(f\"Precision:   {baseline_results.get('precision_macro', 0):.3f}\")\n",
    "print(f\"Recall:      {baseline_results.get('recall_macro', 0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance - top words for each class\n",
    "if hasattr(baseline, 'get_feature_importance'):\n",
    "    importance = baseline.get_feature_importance(top_n=15)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    for idx, (label, features) in enumerate(importance.items()):\n",
    "        words = [f[0] for f in features]\n",
    "        weights = [f[1] for f in features]\n",
    "        \n",
    "        color = '#2ecc71' if label == 'ACC' else '#e74c3c'\n",
    "        axes[idx].barh(words, weights, color=color)\n",
    "        axes[idx].set_xlabel('Weight')\n",
    "        axes[idx].set_title(f'Top Words for {label}', fontsize=14, fontweight='bold')\n",
    "        axes[idx].invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for baseline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "baseline_preds = baseline.predict(test_df['text'].tolist())\n",
    "cm = confusion_matrix(test_df['label'].tolist(), baseline_preds, labels=['ACC', 'OFF'])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['ACC', 'OFF'], yticklabels=['ACC', 'OFF'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Baseline Model Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. BERTić Transformer Model\n\nWe fine-tune BERTić (`classla/bcms-bertic`), a BERT model pre-trained on 8 billion tokens of South Slavic text.\n\n**Our fine-tuned model:** [TeoMatosevic/croatian-hate-speech-bertic](https://huggingface.co/TeoMatosevic/croatian-hate-speech-bertic)\n\nTo download:\n```python\nfrom huggingface_hub import snapshot_download\nsnapshot_download(\"TeoMatosevic/croatian-hate-speech-bertic\", local_dir=\"checkpoints/bertic/best_model\")\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BERTić Model Configuration:\")\n",
    "print(\"=\"*50)\n",
    "print(\"Base model:       classla/bcms-bertic\")\n",
    "print(\"Architecture:     12 layers, 768 hidden dim, 110M params\")\n",
    "print(\"\")\n",
    "print(\"Training Configuration:\")\n",
    "print(\"  Learning rate:  2e-5\")\n",
    "print(\"  Batch size:     16\")\n",
    "print(\"  Epochs:         5\")\n",
    "print(\"  Max length:     256 tokens\")\n",
    "print(\"  Optimizer:      AdamW\")\n",
    "print(\"  Warmup ratio:   0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERTić if available\n",
    "from src.models.bertic import BERTicTrainer\n",
    "\n",
    "bertic_path = Path('checkpoints/bertic/best_model')\n",
    "bertic = None\n",
    "\n",
    "if bertic_path.exists():\n",
    "    print(\"Loading pre-trained BERTić model...\")\n",
    "    try:\n",
    "        bertic = BERTicTrainer()\n",
    "        bertic.load(str(bertic_path))\n",
    "        print(\"BERTić model loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load BERTić: {e}\")\n",
    "        print(\"BERTić results will be shown from saved metrics.\")\n",
    "else:\n",
    "    print(\"BERTić checkpoint not found.\")\n",
    "    print(\"Results shown below are from previous training runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate BERTić on test set\nif bertic is not None:\n    print(\"Evaluating BERTić on test set (this may take a few minutes on CPU)...\")\n    bertic_results = bertic.evaluate(test_df['text'].tolist(), test_df['label'].tolist())\n    report = bertic_results.get('classification_report', {})\n    bertic_results['per_class'] = {\n        'ACC': {\n            'precision': report.get('ACC', {}).get('precision', 0),\n            'recall': report.get('ACC', {}).get('recall', 0),\n            'f1': report.get('ACC', {}).get('f1-score', 0),\n        },\n        'OFF': {\n            'precision': report.get('OFF', {}).get('precision', 0),\n            'recall': report.get('OFF', {}).get('recall', 0),\n            'f1': report.get('OFF', {}).get('f1-score', 0),\n        }\n    }\nelse:\n    print(\"BERTić not loaded — using saved metrics.\")\n    bertic_results = {\n        'accuracy': 0.8127, 'f1_macro': 0.810, 'f1_weighted': 0.813, 'mcc': 0.621,\n        'per_class': {\n            'ACC': {'precision': 0.777, 'recall': 0.803, 'f1': 0.790},\n            'OFF': {'precision': 0.842, 'recall': 0.820, 'f1': 0.831}\n        }\n    }\n\nprint(\"\\nBERTić Model Performance (Test Set):\")\nprint(\"=\"*50)\nprint(f\"Accuracy:    {bertic_results.get('accuracy', 0):.1%}\")\nprint(f\"F1-Macro:    {bertic_results.get('f1_macro', 0):.3f}\")\nprint(f\"F1-Weighted: {bertic_results.get('f1_weighted', 0):.3f}\")\nprint(f\"MCC:         {bertic_results.get('mcc', 0):.3f}\")\nprint(\"\\nPer-Class Performance:\")\nfor cls in ['ACC', 'OFF']:\n    pc = bertic_results['per_class'][cls]\n    print(f\"  {cls}: P={pc['precision']:.3f}, R={pc['recall']:.3f}, F1={pc['f1']:.3f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# BERTić per-class metrics visualization\npc = bertic_results['per_class']\nmetrics_data = {\n    'Class': ['ACC', 'ACC', 'ACC', 'OFF', 'OFF', 'OFF'],\n    'Metric': ['Precision', 'Recall', 'F1-Score', 'Precision', 'Recall', 'F1-Score'],\n    'Value': [\n        pc['ACC']['precision'], pc['ACC']['recall'], pc['ACC']['f1'],\n        pc['OFF']['precision'], pc['OFF']['recall'], pc['OFF']['f1'],\n    ]\n}\nmetrics_df = pd.DataFrame(metrics_data)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(data=metrics_df, x='Class', y='Value', hue='Metric', palette='viridis')\nplt.ylim(0.7, 0.9)\nplt.title('BERTić Per-Class Performance', fontsize=14, fontweight='bold')\nplt.ylabel('Score')\nplt.legend(title='Metric')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "source": "## 7. XLM-RoBERTa Transformer Model\n\nWe fine-tune XLM-RoBERTa (`xlm-roberta-base`), a multilingual transformer pre-trained on 100 languages including Croatian, with 278M parameters.\n\n**Our fine-tuned model:** [TeoMatosevic/croatian-hate-speech-xlm-roberta](https://huggingface.co/TeoMatosevic/croatian-hate-speech-xlm-roberta)\n\nTo download:\n```python\nfrom huggingface_hub import snapshot_download\nsnapshot_download(\"TeoMatosevic/croatian-hate-speech-xlm-roberta\", local_dir=\"checkpoints/xlm_roberta/best_model\")\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"XLM-RoBERTa Model Configuration:\")\nprint(\"=\"*50)\nprint(\"Base model:       xlm-roberta-base\")\nprint(\"Architecture:     12 layers, 768 hidden dim, 278M params\")\nprint(\"\")\nprint(\"Training Configuration:\")\nprint(\"  Learning rate:  2e-5\")\nprint(\"  Batch size:     16\")\nprint(\"  Epochs:         5\")\nprint(\"  Max length:     256 tokens\")\nprint(\"  Optimizer:      AdamW\")\nprint(\"  Loss:           Cross-entropy\")\nprint(\"  Warmup ratio:   0.1\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Load XLM-RoBERTa if available\nfrom src.models.xlm_roberta import XLMRobertaTrainer\n\nxlm_roberta_path = Path('checkpoints/xlm_roberta/best_model')\nxlm_roberta = None\n\nif xlm_roberta_path.exists():\n    print(\"Loading pre-trained XLM-RoBERTa model...\")\n    try:\n        xlm_roberta = XLMRobertaTrainer()\n        xlm_roberta.load(str(xlm_roberta_path))\n        print(\"XLM-RoBERTa model loaded successfully!\")\n    except Exception as e:\n        print(f\"Could not load XLM-RoBERTa: {e}\")\n        print(\"XLM-RoBERTa results will be shown from saved metrics.\")\nelse:\n    print(\"XLM-RoBERTa checkpoint not found.\")\n    print(\"Results shown below are from previous training runs.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Evaluate XLM-RoBERTa on test set\nif xlm_roberta is not None:\n    print(\"Evaluating XLM-RoBERTa on test set (this may take a few minutes on CPU)...\")\n    xlm_results = xlm_roberta.evaluate(test_df['text'].tolist(), test_df['label'].tolist())\nelse:\n    print(\"XLM-RoBERTa not loaded — using saved metrics.\")\n    xlm_results = {\n        'accuracy': 0.748, 'f1_macro': 0.745, 'f1_weighted': 0.748, 'mcc': 0.490,\n    }\n\nprint(\"\\nXLM-RoBERTa Model Performance (Test Set):\")\nprint(\"=\"*50)\nprint(f\"Accuracy:    {xlm_results.get('accuracy', 0):.1%}\")\nprint(f\"F1-Macro:    {xlm_results.get('f1_macro', 0):.3f}\")\nprint(f\"F1-Weighted: {xlm_results.get('f1_weighted', 0):.3f}\")\nprint(f\"MCC:         {xlm_results.get('mcc', 0):.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Model Comparison\n\nComparing all models to highlight the improvement from transformer-based approaches."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Comparison table\ncomparison_data = {\n    'Model': ['Logistic Regression', 'SVM (Linear)', 'XLM-RoBERTa', 'BERTić'],\n    'Accuracy': [0.716, 0.710, 0.748, 0.813],\n    'F1-Macro': [0.711, 0.707, 0.745, 0.810],\n    'F1-Weighted': [0.714, 0.710, 0.748, 0.813],\n    'MCC': [0.423, 0.414, 0.490, 0.621]\n}\ncomparison_df = pd.DataFrame(comparison_data)\n\nprint(\"Model Comparison:\")\nprint(\"=\"*70)\ncomparison_df"
  },
  {
   "cell_type": "code",
   "source": "# Visual comparison\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nmodels = comparison_df['Model']\nx = np.arange(len(models))\nwidth = 0.35\n\n# F1-Macro comparison\nbars1 = axes[0].bar(x, comparison_df['F1-Macro'], width, color=['#3498db', '#3498db', '#e67e22', '#2ecc71'])\naxes[0].set_ylabel('F1-Macro Score')\naxes[0].set_title('F1-Macro Score by Model', fontsize=14, fontweight='bold')\naxes[0].set_xticks(x)\naxes[0].set_xticklabels(models, rotation=15, ha='right')\naxes[0].set_ylim(0.6, 0.9)\nfor bar, val in zip(bars1, comparison_df['F1-Macro']):\n    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n                 f'{val:.3f}', ha='center', va='bottom', fontweight='bold')\n\n# MCC comparison\nbars2 = axes[1].bar(x, comparison_df['MCC'], width, color=['#3498db', '#3498db', '#e67e22', '#2ecc71'])\naxes[1].set_ylabel('MCC Score')\naxes[1].set_title(\"Matthew's Correlation Coefficient by Model\", fontsize=14, fontweight='bold')\naxes[1].set_xticks(x)\naxes[1].set_xticklabels(models, rotation=15, ha='right')\naxes[1].set_ylim(0.3, 0.7)\nfor bar, val in zip(bars2, comparison_df['MCC']):\n    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n                 f'{val:.3f}', ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 9. Interactive Demo\n\nAnalyze any text with our models and lexicon."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def analyze_text(text, baseline_model=None, bertic_model=None, xlm_roberta_model=None, lexicon=None):\n    \"\"\"Analyze text with all available models.\"\"\"\n    print(\"=\"*70)\n    print(f\"INPUT: \\\"{text}\\\"\")\n    print(\"=\"*70)\n    \n    # Lexicon analysis\n    if lexicon:\n        matches = lexicon.find_matches(text)\n        print(\"\\n[LEXICON] Coded Terms:\")\n        if matches:\n            for match in matches:\n                print(f\"  - '{match['term']}' -> {match['coded_meaning']} (Target: {match['target_group']})\")\n        else:\n            print(\"  No coded terms detected.\")\n    \n    # Baseline prediction\n    if baseline_model:\n        pred = baseline_model.predict([text])[0]\n        proba = baseline_model.predict_proba([text])\n        print(f\"\\n[BASELINE] Prediction: {pred}\")\n        if proba is not None:\n            print(f\"  Confidence: {proba[0].max():.1%}\")\n    \n    # XLM-RoBERTa prediction\n    if xlm_roberta_model:\n        try:\n            pred = xlm_roberta_model.predict([text])[0]\n            print(f\"\\n[XLM-RoBERTa] Prediction: {pred}\")\n        except Exception as e:\n            print(f\"\\n[XLM-RoBERTa] Not available: {e}\")\n    \n    # BERTić prediction\n    if bertic_model:\n        try:\n            pred = bertic_model.predict([text])[0]\n            print(f\"\\n[BERTić] Prediction: {pred}\")\n        except Exception as e:\n            print(f\"\\n[BERTić] Not available: {e}\")\n    \n    print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demo with example sentences\ndemo_sentences = [\n    \"Hvala na lijepom komentaru, slažem se s vama.\",\n    \"Inženjeri opet prave probleme u našem gradu.\",\n    \"Svi političari su lopovi i treba ih zatvoriti.\",\n    \"Globalisti žele uništiti našu kulturu i tradiciju.\",\n    \"Ovo je odlična vijest za Hrvatsku!\",\n    \"Ovce će vjerovati u sve što im mediji kažu.\"\n]\n\nprint(\"Demo Analysis of Sample Sentences:\\n\")\nfor sentence in demo_sentences:\n    analyze_text(sentence, baseline_model=baseline, bertic_model=bertic,\n                 xlm_roberta_model=xlm_roberta, lexicon=lexicon)"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 10. Error Analysis\n\nExamining cases where the model makes mistakes."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified examples (baseline)\n",
    "test_texts = test_df['text'].tolist()\n",
    "test_labels = test_df['label'].tolist()\n",
    "baseline_predictions = baseline.predict(test_texts)\n",
    "\n",
    "# Create error analysis dataframe\n",
    "error_df = test_df.copy()\n",
    "error_df['predicted'] = baseline_predictions\n",
    "error_df['correct'] = error_df['label'] == error_df['predicted']\n",
    "\n",
    "misclassified = error_df[~error_df['correct']]\n",
    "print(f\"Baseline Misclassification Analysis:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total test samples: {len(test_df)}\")\n",
    "print(f\"Correctly classified: {len(error_df[error_df['correct']])} ({len(error_df[error_df['correct']])/len(test_df)*100:.1f}%)\")\n",
    "print(f\"Misclassified: {len(misclassified)} ({len(misclassified)/len(test_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample misclassified examples\n",
    "print(\"\\nSample False Positives (ACC predicted as OFF):\")\n",
    "print(\"-\"*60)\n",
    "false_positives = misclassified[(misclassified['label'] == 'ACC') & (misclassified['predicted'] == 'OFF')]\n",
    "for i, row in false_positives.head(3).iterrows():\n",
    "    print(f\"  Text: {row['text'][:80]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nSample False Negatives (OFF predicted as ACC):\")\n",
    "print(\"-\"*60)\n",
    "false_negatives = misclassified[(misclassified['label'] == 'OFF') & (misclassified['predicted'] == 'ACC')]\n",
    "for i, row in false_negatives.head(3).iterrows():\n",
    "    print(f\"  Text: {row['text'][:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 11. Conclusions\n\n### Key Findings\n\n1. **Transformer Superiority**: BERTić significantly outperforms traditional ML baselines, achieving +13.9% F1 improvement over Logistic Regression (0.810 vs 0.711).\n\n2. **Language-Specific vs Multilingual Pre-training**: BERTić (South Slavic pre-training) outperforms XLM-RoBERTa (multilingual pre-training) by a significant margin (F1: 0.810 vs 0.745, McNemar p < 0.001), demonstrating the value of language-specific models for Croatian.\n\n3. **XLM-RoBERTa as Middle Ground**: XLM-RoBERTa achieves +4.8% F1 improvement over baselines (0.745 vs 0.711), offering moderate gains from multilingual transfer learning.\n\n4. **Statistical Significance**: All model differences are statistically significant (McNemar's test, p < 0.05) except Logistic Regression vs SVM (p = 0.497).\n\n5. **Coded Language Detection**: The lexicon of 32 dog whistle terms provides complementary detection for implicit hate speech.\n\n### Practical Applications\n\n- Semi-automated content moderation on Croatian online platforms\n- Reducing manual moderation workload while maintaining accuracy\n- Detecting implicit hate speech through lexicon integration\n\n### Future Work\n\n- Multi-label classification for hate speech subtypes\n- Cross-platform evaluation (social media)\n- Integration of lexicon features with neural models\n- Explainable AI for transparent moderation decisions\n\n---\n\n**Repository:** https://github.com/TeoMatosevic/slur-analysis-model\n\n**Pre-trained Models (HuggingFace):**\n- BERTić: https://huggingface.co/TeoMatosevic/croatian-hate-speech-bertic\n- XLM-RoBERTa: https://huggingface.co/TeoMatosevic/croatian-hate-speech-xlm-roberta\n- Baseline: https://huggingface.co/TeoMatosevic/croatian-hate-speech-baseline"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"\\n\" + \"=\"*60)\nprint(\"PROJECT SHOWCASE COMPLETE\")\nprint(\"=\"*60)\nprint(\"\\nThis notebook demonstrated all major components of the\")\nprint(\"Croatian Hate Speech Detection project.\")\nprint(\"\\nFor more information, see:\")\nprint(\"  - docs/paper.md (Academic paper)\")\nprint(\"  - README.md (Project documentation)\")\nprint(\"  - src/demo.py (Interactive demo script)\")\nprint(\"\\nPre-trained Models (HuggingFace):\")\nprint(\"  - BERTić: huggingface.co/TeoMatosevic/croatian-hate-speech-bertic\")\nprint(\"  - XLM-RoBERTa: huggingface.co/TeoMatosevic/croatian-hate-speech-xlm-roberta\")\nprint(\"  - Baseline: huggingface.co/TeoMatosevic/croatian-hate-speech-baseline\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}